{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why CNN and not Regular Neural Nets\n",
    "\n",
    "**1. Regular Neural Nets don’t scale well to full images**\n",
    "\n",
    "In MNIST dataset,images are only of size 28x28x1 (28 wide, 28 high, 1 color channels), so a single fully-connected neuron in a first hidden layer of a regular Neural Network would have 28x28x1 = 786 weights. This amount still seems manageable,\n",
    "\n",
    "**But what if we move to larger images.**\n",
    "\n",
    "For example, an image of more respectable size, e.g. 200x200x3, would lead to neurons that have 200x200x3 = 120,000 weights. Moreover, we would almost certainly want to have several such neurons, so the parameters would add up quickly! Clearly, this full connectivity is wasteful and the huge number of parameters would quickly lead to overfitting.\n",
    "\n",
    "**2.Parameter Sharing**\n",
    "<br>\n",
    "A feature detector that is useful in one part of the image is probably useful in another part of the image.Thus CNN are good in capturing translation invariance. \n",
    "\n",
    "**Sparsity of connections**\n",
    "In each layer,each output value depends only on a small number of inputs.This makes CNN networks easy to train on smaller training datasets and is less prone to overfitting.\n",
    "\n",
    "**2.3D volumes of neurons.**\n",
    "Convolutional Neural Networks take advantage of the fact that the input consists of images and they constrain the architecture in a more sensible way. In particular, unlike a regular Neural Network, the layers of a ConvNet have neurons arranged in 3 dimensions: width, height, depth.\n",
    "\n",
    "![](images/ffnvscnn.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution\n",
    "\n",
    "\n",
    "In purely mathematical terms, convolution is a function derived from two given functions by integration which expresses how the shape of one is modified by the other.\n",
    "\n",
    "However we are interested in understanding the actual convolution operation in the context of neural networks.\n",
    "\n",
    "**An intuitive understanding of Convolution**\n",
    "<br>\n",
    "Convolution is an operation done  to extract features from the images as these features will be used by the network to learn about a particular image.In the case of a dog image,the feature could be the shape of a nose or the shape of an eye which will help the network later to identify an image as a dog.\n",
    "\n",
    "Convolution operation is performed with the help of the following three elements:\n",
    "\n",
    "**1.Input Image-** The image to convolve on\n",
    "\n",
    "**2.Feature Detector/Kernel/Filter-** They are the bunch of numbers in a matrix form intended to extract features from an image.They can be 1dimensional ,2-dimensional or 3-dimensional\n",
    "\n",
    "**3.Feature Map/Activation Map-** The resultant of the convolution operation performed between image and feature detector gives a Feature Map.\n",
    "\n",
    "![](images/convolution.PNG)\n",
    "\n",
    "![](https://i1.wp.com/timdettmers.com/wp-content/uploads/2015/03/convolution.png)\n",
    "\n",
    "\n",
    "![](images/feature1.PNG)\n",
    "![](images/feature2.PNG)\n",
    "![](images/feature3.PNG)\n",
    "\n",
    "\n",
    "\n",
    "**Convolution Operation**\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1200/1*VVvdh-BUKFh2pwDD0kPeRA@2x.gif)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Another way to look at it**\n",
    "\n",
    "![](images/conv3d1.png)\n",
    "\n",
    "\n",
    "Let say we have an input of $6 x 6$  and a filter size $3 x 3$ \n",
    "\n",
    "**Feature map is of size   $4 x 4$**\n",
    "![](images/conv1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Convolution over Volume**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**What if our input image has more than one channel?**\n",
    "\n",
    "Let say we have an input of $6 x 6 x 3$  and a filter size $3 x 3 x 3$ \n",
    "\n",
    "**Feature map is of size   $4 x 4$**\n",
    "\n",
    "![](images/conv3d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Operation with Multiple Filters\n",
    "\n",
    "Let say we have an input of $6 x 6 x 3$  and we use two filters size $3 x 3$ \n",
    "\n",
    "**Feature map is of size   $4 x 4 x 2$**\n",
    "\n",
    "![](images/conv2.PNG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-11T15:44:50.785819Z",
     "start_time": "2019-02-11T15:44:50.759888Z"
    }
   },
   "source": [
    "# **General Representation**\n",
    "<br>\n",
    "$$Input Image [n(h)*n(w)*n(c)]-Filter-[f(h)*f(w)*n(c)],n(c')--Feature Map--[(n-f+1)*(n-f+1)*n(c')]$$\n",
    "\n",
    "**$n(h)$**-height of the image\n",
    "\n",
    "**$n(w)$**-width of the image\n",
    "\n",
    "**$n(c)$**-number of channel in an image\n",
    "\n",
    "**$f(h)$**-height of the filter\n",
    "\n",
    "**$f(w)$**-width of the filter\n",
    "\n",
    "**$f(c')$**-no of the filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Convolution layer\n",
    "\n",
    "![](images/convlayer.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Strides\n",
    "\n",
    "![](images/stride.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding\n",
    "\n",
    "![](images/padding.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pooling\n",
    "\n",
    "![](images/pooling.PNG)\n",
    "\n",
    "**Why we do Pooling?**\n",
    "\n",
    "The idea of max pooling is:\n",
    "1. to reduce the size of representation in such a way that we carry along those features which speaks louder in the image \n",
    "2. to lower the number of parameters to be computed,speeding the computation \n",
    "3. to make some of the features that detects significant things a bit more robust.\n",
    "\n",
    "\n",
    "**Analogy that I like to draw**\n",
    "![](https://qph.fs.quoracdn.net/main-qimg-0eb448b5633a1ff511ac2956f032f816-c)\n",
    "\n",
    "\n",
    "A good analogy to draw here would be to look into the history of shapes of pyramid.\n",
    "\n",
    "The Greek pyramid is the one without max pooling whereas the Mesopotamian pyramid is with max pooling involved where we are loosing more information but making our network simpler than the other one.\n",
    "\n",
    "**But don't we end up loosing information by max pooling?**\n",
    "\n",
    "Yes we do but the question we need to ask is how much information we can afford to loose without impacting much on the model prediction. \n",
    "\n",
    "Perhaps the criteria to choose how often(after how many convolutions) and at what part of the network (at the beginning or at the mid or at the end of the network) to use max pooling depends completely on what this network is being used for.\n",
    "\n",
    "For eg: \n",
    "1. Cats vs Dogs\n",
    "2. Identify the age of a person\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-11T17:12:06.494028Z",
     "start_time": "2019-02-11T17:12:06.482062Z"
    }
   },
   "source": [
    "# **General Representation-Updated**\n",
    "<br>\n",
    "\n",
    "**Including Padding and Stride**\n",
    "\n",
    "$$Input Image [n(h)*n(w)*n(c)]-Filter-[f(h)*f(w)*n(c)],n(c')--Feature Map--[((n-f+2p)/s+1)*((n-f+2p)/s+1)*n(c')]$$\n",
    "\n",
    "**$n(h)$**-height of the image\n",
    "\n",
    "**$n(w)$**-width of the image\n",
    "\n",
    "**$n(c)$**-number of channel in an image\n",
    "\n",
    "**$f(h)$**-height of the filter\n",
    "\n",
    "**$f(w)$**-width of the filter\n",
    "\n",
    "**$f(c')$**-no of the filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/network.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-11T17:17:19.153259Z",
     "start_time": "2019-02-11T17:17:18.724558Z"
    }
   },
   "source": [
    "# Examples\n",
    "\n",
    "**Input volume:** 32x32x3\n",
    "<br>\n",
    "10 5x5 filters with stride 1, pad 2\n",
    "<br>\n",
    "Output volume size: ?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "32x32x10\n",
    "\n",
    "\n",
    "\n",
    "**Input volume:** 32x32x3\n",
    "<br>\n",
    "10 5x5 filters with stride 1, pad 2\n",
    "<br>\n",
    "\n",
    "Number of parameters in this layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 x 1 Convolution\n",
    "\n",
    "At first,the idea of using 1x1 filter seems to not make sense as  1x1 convolution is just multiplying  by numbers.We will not be learning any feature here.\n",
    "\n",
    "But wait... What if we have a layer with dimension 32x32x196,here 196 is the number of channels and we want to do convolution\n",
    "\n",
    "So 1x1x192 convolution will do the work of dimensionality reduction by looking at each of the 196 different positions  and it will do the element wise product and give out one number.Using multiple such filters say 32 will give 32 variations of this number.\n",
    "\n",
    "![](images/1x1.gif)\n",
    "**Why do we use 1x1 filter**\n",
    "\n",
    "1. 1x 1 filter can help in shrinking the number of channels or increasing the number of channels without changing the height and width of the layer.\n",
    "\n",
    "2. It adds nonlinearity in the network which is useful in some of the architectures like Inception network.\n",
    "\n",
    "\n",
    "#![](https://cdn-images-1.medium.com/max/1600/1*KdLQiGlPWSxYJ-dvYM3tyQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Receptive Field\n",
    "\n",
    "The receptive field is defined as the region in the input space that a particular CNN’s feature is looking at (i.e. be affected by). A receptive field of a feature can be described by its center location and its size.\n",
    "\n",
    "![](images/receptive_field.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to remember\n",
    "\n",
    "1. **Filter** will always have the same number of channel as the image.\n",
    "\n",
    "2. **Convolving** gives a **2 D feature map** although our image and kernel used are of 3 dimensional\n",
    "\n",
    "3. **Padding** -Preserves the feature size\n",
    "\n",
    "3. **Pooling Operation**- Reduces the filter size by half\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[Convolution](https://indoml.com/2018/03/07/student-notes-convolutional-neural-networks-cnn-introduction/)\n",
    "<br>\n",
    "[Max Pool](https://wiseodd.github.io/techblog/2016/07/18/convnet-maxpool-layer/)\n",
    "<br>\n",
    "[Standford Slides](http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture05.pdf)\n",
    "<br>\n",
    "[Standford Blog](http://cs231n.github.io/convolutional-networks/)\n",
    "<br>\n",
    "[An intuitive guide to Convolutional Neural Networks](https://medium.freecodecamp.org/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050)\n",
    "<br>\n",
    "[Convolutional Neural Networks](https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2)\n",
    "<br>\n",
    "[Understanding of Convolutional Neural Network](https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148)\n",
    "<br>\n",
    "[Receptive Feild Calculation](https://medium.com/mlreview/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807)\n",
    "<br>\n",
    "[Understanding Convolution in Deep Learning](http://timdettmers.com/2015/03/26/convolution-deep-learning/)\n",
    "<br>\n",
    "[Visualize Image Kernel](http://setosa.io/ev/image-kernels/)\n",
    "<br>\n",
    "[Visualizing and Understanding Convolution Networks](https://arxiv.org/abs/1311.2901)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "284px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
