{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Document Frequency\n",
    "\n",
    "![](images/dtm.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF- IDF Term Frequency Inverse Document Frequency\n",
    "\n",
    "Words that appear in many documents are probably less meaningful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We are interested in obtaining VXD type matrix where V is the vocabulary size and D is the Vector Dimensionality\n",
    " \n",
    " ![](images/vxd.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "Feature vector representing a word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Analogies\n",
    "\n",
    "There is no concept of word analogies with algorithms like word2vec and Glove.They just suddenly emerge out of the model and training process.\n",
    "\n",
    "![](images/word_analogies.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Word Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Metric\n",
    "\n",
    "1. Euclidean\n",
    "2. Cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove Analogies\n",
    "\n",
    "Get glove.6B.50d file for word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T07:42:10.072028Z",
     "start_time": "2019-12-08T07:42:06.457921Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T07:42:10.726090Z",
     "start_time": "2019-12-08T07:42:10.703854Z"
    }
   },
   "outputs": [],
   "source": [
    "#Euclidean\n",
    "def dist1(a, b):\n",
    "    return np.linalg.norm(a - b)\n",
    "#Cosine\n",
    "def dist2(a, b):\n",
    "    return 1 - a.dot(b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T07:42:12.322976Z",
     "start_time": "2019-12-08T07:42:12.318989Z"
    }
   },
   "outputs": [],
   "source": [
    "# pick a distance type\n",
    "dist, metric = dist2, 'cosine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T07:42:13.907588Z",
     "start_time": "2019-12-08T07:42:13.900607Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_analogies(w1, w2, w3):\n",
    "  for w in (w1, w2, w3):\n",
    "    if w not in word2vec:\n",
    "      print(\"%s not in dictionary\" % w)\n",
    "      return\n",
    "\n",
    "  king = word2vec[w1]\n",
    "  man = word2vec[w2]\n",
    "  woman = word2vec[w3]\n",
    "  v0 = king - man + woman\n",
    "\n",
    "  distances = pairwise_distances(v0.reshape(1, D), embedding, metric=metric).reshape(V)\n",
    "  idxs = distances.argsort()[:4]\n",
    "  for idx in idxs:\n",
    "    word = idx2word[idx]\n",
    "    if word not in (w1, w2, w3): \n",
    "      best_word = word\n",
    "      break\n",
    "\n",
    "  print(w1, \"-\", w2, \"=\", best_word, \"-\", w3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T07:42:15.453214Z",
     "start_time": "2019-12-08T07:42:15.448224Z"
    }
   },
   "outputs": [],
   "source": [
    "def nearest_neighbors(w, n=5):\n",
    "  if w not in word2vec:\n",
    "    print(\"%s not in dictionary:\" % w)\n",
    "    return\n",
    "\n",
    "  v = word2vec[w]\n",
    "  distances = pairwise_distances(v.reshape(1, D), embedding, metric=metric).reshape(V)\n",
    "  idxs = distances.argsort()[1:n+1]\n",
    "  print(\"neighbors of: %s\" % w)\n",
    "  for idx in idxs:\n",
    "    print(\"\\t%s\" % idx2word[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T09:49:08.467483Z",
     "start_time": "2019-06-10T09:48:56.784817Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors...\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Loading word vectors...')\n",
    "word2vec = {}\n",
    "embedding = []\n",
    "idx2word = []\n",
    "with open('glove.6B.50d.txt', encoding='utf-8') as f:\n",
    "  # is just a space-separated text file in the format:\n",
    "  # word vec[0] vec[1] vec[2] ...\n",
    "  for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vec = np.asarray(values[1:], dtype='float32')\n",
    "    word2vec[word] = vec\n",
    "    embedding.append(vec)\n",
    "    idx2word.append(word)\n",
    "print('Found %s word vectors.' % len(word2vec))\n",
    "embedding = np.array(embedding)\n",
    "V, D = embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T09:49:08.753717Z",
     "start_time": "2019-06-10T09:49:08.471474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king - man = queen - woman\n"
     ]
    }
   ],
   "source": [
    "find_analogies('king', 'man', 'woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T09:49:09.111995Z",
     "start_time": "2019-06-10T09:49:08.756710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "france - paris = britain - london\n"
     ]
    }
   ],
   "source": [
    "find_analogies('france', 'paris', 'london')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Analogies\n",
    "\n",
    "Get GoogleNews-vectors-negative300.bin file for word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T07:42:20.322716Z",
     "start_time": "2019-12-08T07:42:20.317729Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T07:45:47.202546Z",
     "start_time": "2019-12-08T07:42:21.546716Z"
    }
   },
   "outputs": [],
   "source": [
    "word_vectors = KeyedVectors.load_word2vec_format(\n",
    "  'GoogleNews-vectors-negative300.bin',\n",
    "  binary=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T07:46:33.296740Z",
     "start_time": "2019-12-08T07:46:33.289761Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_analogies(w1, w2, w3):\n",
    "  r = word_vectors.most_similar(positive=[w1, w3], negative=[w2])\n",
    "  print(\"%s - %s = %s - %s\" % (w1, w2, r[0][0], w3))\n",
    "\n",
    "def nearest_neighbors(w):\n",
    "  r = word_vectors.most_similar(positive=[w])\n",
    "  print(\"neighbors of: %s\" % w)\n",
    "  for word, score in r:\n",
    "    print(\"\\t%s\" % word)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T07:48:44.777050Z",
     "start_time": "2019-12-08T07:46:34.506628Z"
    }
   },
   "source": [
    "print(find_analogies('king', 'man', 'woman'))\n",
    "print(find_analogies('france', 'paris', 'london'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "find_analogies('france', 'paris', 'rome')\n",
    "find_analogies('paris', 'france', 'italy')\n",
    "find_analogies('france', 'french', 'english')\n",
    "find_analogies('japan', 'japanese', 'chinese')\n",
    "find_analogies('japan', 'japanese', 'italian')\n",
    "find_analogies('japan', 'japanese', 'australian')\n",
    "find_analogies('december', 'november', 'june')\n",
    "find_analogies('miami', 'florida', 'texas')\n",
    "find_analogies('einstein', 'scientist', 'painter')\n",
    "find_analogies('china', 'rice', 'bread')\n",
    "find_analogies('man', 'woman', 'she')\n",
    "find_analogies('man', 'woman', 'aunt')\n",
    "find_analogies('man', 'woman', 'sister')\n",
    "find_analogies('man', 'woman', 'wife')\n",
    "find_analogies('man', 'woman', 'actress')\n",
    "find_analogies('man', 'woman', 'mother')\n",
    "find_analogies('heir', 'heiress', 'princess')\n",
    "find_analogies('nephew', 'niece', 'aunt')\n",
    "find_analogies('france', 'paris', 'tokyo')\n",
    "find_analogies('france', 'paris', 'beijing')\n",
    "find_analogies('february', 'january', 'november')\n",
    "find_analogies('france', 'paris', 'rome')\n",
    "find_analogies('paris', 'france', 'italy')\n",
    "\n",
    "nearest_neighbors('king')\n",
    "nearest_neighbors('france')\n",
    "nearest_neighbors('japan')\n",
    "nearest_neighbors('einstein')\n",
    "nearest_neighbors('woman')\n",
    "nearest_neighbors('nephew')\n",
    "nearest_neighbors('february')\n",
    "nearest_neighbors('rome')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "When you're dealing with words in text, you end up with tens of thousands of word classes to analyze; one for each word in a vocabulary. Trying to one-hot encode these words is massively inefficient because most values in a one-hot vector will be set to zero. So, the matrix multiplication that happens in between a one-hot input vector and a first, hidden layer will result in mostly zero-valued hidden outputs.\n",
    "\n",
    "To solve this problem and greatly increase the efficiency of our networks, we use what are called **embeddings**. Embeddings are just a fully connected layer like you've seen before. We call this layer the embedding layer and the weights are embedding weights. We skip the multiplication into the embedding layer by instead directly grabbing the hidden layer values from the weight matrix. We can do this because the multiplication of a one-hot encoded vector with a matrix returns the row of the matrix corresponding the index of the \"on\" input unit.\n",
    "\n",
    "![](images/embedding.PNG)\n",
    "\n",
    "Instead of doing the matrix multiplication, we use the weight matrix as a lookup table. We encode the words as integers, for example \"heart\" is encoded as 958, \"mind\" as 18094. Then to get hidden layer values for \"heart\", you just take the 958th row of the embedding matrix. This process is called an **embedding lookup** and the number of hidden units is the **embedding dimension**.\n",
    " \n",
    "There is nothing magical going on here. The embedding lookup table is just a weight matrix. The embedding layer is just a hidden layer. The lookup is just a shortcut for the matrix multiplication. The lookup table is trained just like any weight matrix.\n",
    "\n",
    "Embeddings aren't only used for words of course. You can use them for any model where you have a massive number of classes. A particular type of model called **Word2Vec** uses the embedding layer to find vector representations of words that contain semantic meaning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "[Word2Vec](https://arxiv.org/pdf/1310.4546.pdf)\n",
    "[Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)\n",
    "\n",
    "Word2Vec is one of the most popular technique to learn word embeddings using shallow neural network.\n",
    "\n",
    "We’re going to train a simple neural network with a single hidden layer to perform a certain task, but then we’re not actually going to use that neural network for the task we trained it on! Instead, the goal is actually just to learn the weights of the hidden layer–we’ll see that these weights are actually the “word vectors” that we’re trying to learn.\n",
    "\n",
    "Different model architectures that can be used with Word2Vec \n",
    "\n",
    "1. CBOW Neural Network Model\n",
    "2. Skipgram Neural Network Model\n",
    "\n",
    "![](images/cbow_skipgram.PNG)\n",
    "\n",
    "\n",
    "Different ways to train Word2vec Model:\n",
    "1. Heirarchial Softmax\n",
    "2. Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skipgram\n",
    "\n",
    "![](images/skipgram_g.PNG)\n",
    "\n",
    "\n",
    "**eg: The quick brown fox jumps over the lazy dog**\n",
    "![](images/skipgram.PNG)\n",
    "\n",
    "\n",
    "We’re going to train the neural network to do the following. Given a specific word in the middle of a sentence (the input word), look at the words **nearby** and pick one at random. The network is going to tell us the probability for every word in our vocabulary of being the “nearby word” that we chose.\n",
    "\n",
    "**nearby** - there is actually a \"window size\" parameter to the algorithm. A typical window size might be 5, meaning 5 words behind and 5 words ahead (10 in total).\n",
    "\n",
    "We’ll train the neural network to do this by feeding it word pairs found in our training documents.\n",
    "The network is going to learn the statistics from the number of times each pairing shows up. So, for example, the network is probably going to get many more training samples of (“Soviet”, “Union”) than it is of (“Soviet”, “Sasquatch”). When the training is finished, if you give it the word “Soviet” as input, then it will output a much higher probability for “Union” or “Russia” than it will for “Sasquatch”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skipgram Model Details\n",
    "\n",
    "Skipgram architecture consists of:\n",
    "1. [Embedding layer / Hidden Layer](https://pytorch.org/docs/stable/nn.html#embedding)\n",
    "\n",
    "An Embedding layer takes in a number of inputs, importantly:\n",
    "* **num_embeddings** – the size of the dictionary of embeddings, or how many rows you'll want in the embedding weight matrix\n",
    "* **embedding_dim** – the size of each embedding vector; the embedding dimension.(300 features is what Google used in their published model trained on the Google news dataset (you can download it from here)\n",
    "\n",
    "\n",
    "\n",
    "2. Softmax Output Layer<br>\n",
    "The output layer will have output neuron (one per word in our vocabulary) will produce an output between 0 and 1, and the sum of all these output values will add up to 1.\n",
    "\n",
    "\n",
    "![](images/skip_gram_arch.PNG)\n",
    "\n",
    "**Working**\n",
    "1. The input words are passed in as batches of input word tokens. \n",
    "2. This will go into a hidden layer of linear units (our embedding layer). \n",
    "3. Then, finally into a softmax output layer. We'll use the softmax layer to make a prediction about the context words by sampling, as usual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Word2Vec Model\n",
    "\n",
    "Word2Vec network is a huge network in terms of parameters.\n",
    "Say we had word vectors with 300 components, and a vocabulary of 10,000 words. Recall that the neural network had two weight matrices–a hidden layer and output layer. Both of these layers would have a weight matrix with 300 x 10,000 = 3 million weights each!\n",
    "\n",
    "Hence to reduce compute burden of the training process,the research paper proposed following two innovations:\n",
    "1. **Subsampling frequent words** to decrease the number of training examples.\n",
    "2. Modifying the optimization objective with a technique they called **Negative Sampling**,instead of normal cross entropy which causes each training sample to update only a small percentage of the model’s weights which makes training the network very inefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subsampling\n",
    "\n",
    "Words that show up often such as \"the\", \"of\", and \"for\" don't provide much context to the nearby words. If we discard some of them, we can remove some of the noise from our data and in return get faster training and better representations. This process is called subsampling by Mikolov. For each word $w_i$ in the training set, we'll discard it with probability given by\n",
    "\n",
    "$$ P(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}} $$\n",
    "\n",
    "where $t$ is a threshold parameter and $f(w_i)$ is the frequency of word $w_i$ in the total dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Negative Sampling\n",
    "\n",
    "As discussed above,the the skip-gram neural network has a tremendous number of weights, all of which would be updated slightly by every one of our millions or billions of training samples!\n",
    "\n",
    "Negative sampling addresses this by having each training sample only modify a small percentage of the weights, rather than all of them.\n",
    "\n",
    "With negative sampling, we are instead going to randomly select just a small number of “negative” words (let’s say 5) to update the weights for. (In this context, a “negative” word is one for which we want the network to output a 0 for). We will also still update the weights for our “positive” word (which is the word “quick” in our current example).\n",
    "\n",
    "***The paper says that selecting 5-20 words works well for smaller datasets, and you can get away with only 2-5 words for large datasets.***\n",
    "\n",
    "\n",
    "\n",
    "#### Selecting Negative Samples\n",
    "\n",
    "The “negative samples” (that is, the 5 output words that we’ll train to output 0) are selected using a “unigram distribution”, where more frequent words are more likely to be selected as negative samples.\n",
    "\n",
    "**Modification in Cost Function**\n",
    "\n",
    "We change the loss function to only care about correct example and a small amount of wrong examples.\n",
    "![](https://render.githubusercontent.com/render/math?math=-%20%5Clarge%20%5Clog%7B%5Csigma%5Cleft%28u_%7Bw_O%7D%5Chspace%7B0.001em%7D%5E%5Ctop%20v_%7Bw_I%7D%5Cright%29%7D%20-%0A%5Csum_i%5EN%20%5Cmathbb%7BE%7D_%7Bw_i%20%5Csim%20P_n%28w%29%7D%5Clog%7B%5Csigma%5Cleft%28-u_%7Bw_i%7D%5Chspace%7B0.001em%7D%5E%5Ctop%20v_%7Bw_I%7D%5Cright%29%7D&mode=display)\n",
    "\n",
    "First part of the Loss function:\n",
    "![](https://render.githubusercontent.com/render/math?math=%5Clarge%20%5Clog%7B%5Csigma%5Cleft%28u_%7Bw_O%7D%5Chspace%7B0.001em%7D%5E%5Ctop%20v_%7Bw_I%7D%5Cright%29%7D&mode=display)\n",
    "we take the log-sigmoid of the inner product of the output word vector  and the input word vector.\n",
    "\n",
    "Second part of the Loss function:\n",
    "\n",
    "\n",
    "let's first look at \n",
    "\n",
    "$$\\large \\sum_i^N \\mathbb{E}_{w_i \\sim P_n(w)}$$ \n",
    "\n",
    "This means we're going to take a sum over words $w_i$ drawn from a noise distribution $w_i \\sim P_n(w)$. The noise distribution is basically our vocabulary of words that aren't in the context of our input word. In effect, we can randomly sample words from our vocabulary to get these words. $P_n(w)$ is an arbitrary probability distribution though, which means we get to decide how to weight the words that we're sampling. This could be a uniform distribution, where we sample all words with equal probability. Or it could be according to the frequency that each word shows up in our text corpus, the unigram distribution $U(w)$. The authors found the best distribution to be $U(w)^{3/4}$, empirically. The power makes less frequent words be sampled more often\n",
    "\n",
    "Finally, in \n",
    "\n",
    "$$\\large \\log{\\sigma\\left(-u_{w_i}\\hspace{0.001em}^\\top v_{w_I}\\right)},$$ \n",
    "\n",
    "we take the log-sigmoid of the negated inner product of a noise vector with the input vector. \n",
    "\n",
    "![](images/neg_sampling_loss.PNG)\n",
    "\n",
    "To give you an intuition for what we're doing here, remember that the sigmoid function returns a probability between 0 and 1. The first term in the loss pushes the probability that our network will predict the correct word $w_O$ towards 1. In the second term, since we are negating the sigmoid input, we're pushing the probabilities of the noise words towards 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Implementation codes\n",
    "\n",
    "1. Numpy implementation\n",
    "2. Pytorch Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Vector for Word Representation(GloVe)\n",
    "\n",
    "[GLoVe](https://www.aclweb.org/anthology/D14-1162)\n",
    "\n",
    "**Motivation**\n",
    "\n",
    "The Word2Vec -***context window-based methods*** suffer from the disadvantage of not learning from the global corpus statistics. As a result, repetition and large-scale patterns may not be learned as well with these models.\n",
    "\n",
    "This method combines elements from the two main word embedding models which existed when GloVe was proposed: \n",
    "1. Count based approach - Global matrix factorization \n",
    "2. Direct Prediction - Local context window methods\n",
    "\n",
    "\n",
    "![](images/count_based_vs_direct_pred.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Matrix Factorization - Count based approach\n",
    "\n",
    "It is the process of using matrix factorization methods from linear algebra to perform rank reduction on a large term-frequency matrix.\n",
    "These matrices can be represented as :\n",
    "1. Term-Document Frequency - rows are words and the columns are documents (or sometimes paragraphs)\n",
    "2. Term-Term Frequencies   - words on both axes and measure co-occurrence\n",
    "\n",
    "**Latent semantic analysis (LSA)**\n",
    "It is a technique in natural language processing for analyzing relationships between a set of documents and the terms they contain by applying **Global matrix factorization** to term-document frequency matrices.\n",
    "In **LSA** the high-dimensional matrix is reduced via **singular value decomposition (SVD).**\n",
    "\n",
    "![](images/svd.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Context Window Direct prediction\n",
    "\n",
    "1. **Skip-gram model**<br>\n",
    "By passing a window over the corpus line-by-line and learning to predict either the surroundings of a given word  \n",
    "2. **Continuous Bag of Words Model (CBOW)**<br>\n",
    "Predict a word given its surroundings.\n",
    "Note the bag-of-words problem is often shortened to “CBOW”.\n",
    "\n",
    "**Skip-gram**: works well with small amount of the training data, represents well even rare words or phrases.<br>\n",
    "**CBOW**: several times faster to train than the skip-gram, slightly better accuracy for the frequent words.\n",
    "\n",
    "\n",
    "![](images/cbow_skipgram.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe embedding generation algorithm \n",
    "\n",
    "GloVe technique improves on these previous methods by making changes in the following:\n",
    "\n",
    "1. **Co-occurance Probabilities**<br>\n",
    "Instead of learning the raw co-occurrence probabilities, it may make more sense to learn ratios of these co-occurrence probabilities, which seem to better discriminate subtleties in term-term relevance.\n",
    "\n",
    "To illustrate this, we borrow an example from their paper: suppose we wish to study the relationship between two words, i = ice and j = steam. We’ll do this by examining the co-occurrence probabilities of these words with various “probe” words.\n",
    "\n",
    "Co-occurrence probability of an arbitrary word i with an arbitrary word j to be the probability that word j appears in the context of word i.\n",
    "![](images/co-occurance_matrix.PNG)\n",
    "***X_i*** is defined as the number of times any word appears in the context of word i, so it’s defined as the sum over all words k of the number of times word k occurs in the context of word i.\n",
    "\n",
    "Let us take few probe words and see how does the ratio appears:\n",
    "1. If we choose a probe word k = solid which is closely related to i = ice but not to j = steam, we expect the ratio P_{ik}/P_{jk} of co-occurrence probabilities to be large\n",
    "2. If we choose a probe word k = gas we would expect the same ratio to be small, since steam is more closely related to gas than ice is.\n",
    "3. If we choose a probe word k = water , which are closely related to both ice and steam, but not more to one than the other ,we expect our ratio to be close to 1 since there shouldn’t be any bias to one of ice or steam\n",
    "4. If we choose a probe word k = fashion ,which are not closely related to either of the words in question, we expect our ratio to be close to 1 since there shouldn’t be any bias to one of ice or steam\n",
    "\n",
    "![](images/co-occurance_ex.PNG)\n",
    "\n",
    "Noting that the ratio P<sub>ik</sub> /P<sub>jk</sub> depends on three words i, j, and k,\n",
    "the most general model takes the form,\n",
    "![](images/naive_form.PNG)\n",
    "\n",
    "***In this equation, the right-hand side is extracted from the corpus, and F may depend on some as-of-yet unspecified parameters. The number of possibilities for F is vast, but by enforcing a few desiderata we can select a\n",
    "unique choice***.\n",
    "We have two word vectors which we’d like to discriminate between, and a context word vector which is used to this effect.So to encode information about the ratios between two words, the authors suggest using vector differences as inputs to our function\n",
    "**Vector Difference Model**\n",
    "![](images/vector_difference_model.PNG)\n",
    "\n",
    "So now,vector difference of the two words **i** and **j** we’re comparing as an input instead of both of these words individually, since our output is a ratio between their co-occurrence probabilities with the context word.\n",
    "\n",
    "Now we have two arguments, the context word vector, and the vector difference of the two words we’re comparing. Since the authors wish to take scalar values to scalar values (note the ratio of probabilities is a scalar), the dot product of these two arguments is taken\n",
    "\n",
    "![](images/scalar_input_model.PNG)\n",
    "\n",
    "Next, note that for word-word co-occurrence matrices, the distinction between a word and a context word is arbitrary and that we are free to exchange the two roles.Our final model should be invariant under this relabeling, but above equation is not.\n",
    "\n",
    "After applying Homomorphism condition:\n",
    "![](images/homomorphism.PNG)\n",
    "\n",
    "we arrive at the equation as in the paper\n",
    "![](images/glove_homo.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T08:34:07.711695Z",
     "start_time": "2019-12-08T08:34:07.501755Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T08:34:08.543273Z",
     "start_time": "2019-12-08T08:34:08.429433Z"
    }
   },
   "outputs": [],
   "source": [
    "def logit(x):\n",
    "    return np.log(x/(1-x))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/ (1+ np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T08:34:09.597418Z",
     "start_time": "2019-12-08T08:34:09.132212Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# p(y=1/x)= Sigmoid(logit(p))\n",
    "sigmoid(logit(0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T08:34:10.509324Z",
     "start_time": "2019-12-08T08:34:10.394789Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19999999999999996"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#p(y=0/x) = sigmoid(-logit(p))\n",
    "sigmoid(-logit(0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T08:34:11.676189Z",
     "start_time": "2019-12-08T08:34:11.544024Z"
    }
   },
   "outputs": [],
   "source": [
    "# p(y=1/x)= Sigmoid(logit(p))  , p(y=0/x) = sigmoid(-logit(p))\n",
    "# sigmoid(logit) + sigmoid(-logit) =1\n",
    "#Inverse of logit is sigmoid i.e p = sigmoid(logit(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid(-x)= 1-sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T08:34:13.292910Z",
     "start_time": "2019-12-08T08:34:13.249722Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n",
      "0.19999999999999996\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(sigmoid(logit(0.8)))\n",
    "print(sigmoid(-logit(0.8)))\n",
    "print(sigmoid(logit(0.8))+sigmoid(-logit(0.8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T08:34:14.282372Z",
     "start_time": "2019-12-08T08:34:14.277385Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.3411538747320879\n",
      "-0.744396660073571\n",
      "1.1154973260213485\n"
     ]
    }
   ],
   "source": [
    "print(np.log(sigmoid(0.9)))\n",
    "print(np.log(sigmoid(-0.1)))\n",
    "print(-(np.log(sigmoid(0.8))+np.log(sigmoid(-0.1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "1. [Bayes Rule](https://math.stackexchange.com/questions/1037327/extended-bayes-theorem-pa-b-c-d-constructing-a-bayesian-network)\n",
    "2. [Word2Vec from Chris McCormick ](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
    "3. [Word Embedding](https://monkeylearn.com/blog/word-embeddings-transform-text-numbers/)\n",
    "4. [Word2Vec](https://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html)\n",
    "5. [First Word2Vec paper](https://arxiv.org/pdf/1301.3781.pdf) from Mikolov et al.\n",
    "6. [Video: Intuition & Use-Cases of Embeddings in NLP & beyond](http://jalammar.github.io/)\n",
    "7. [Neural Information Processing Systems, paper](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) with improvements for Word2Vec also from Mikolov et al.\n",
    "8. [Skipgram-Pytorch](https://github.com/udacity/deep-learning-v2-pytorch/blob/master/word2vec-embeddings/Negative_Sampling_Solution.ipynb)\n",
    "9. [Word2Vec-Pytorch](https://github.com/dthiagarajan/word2vec-pytorch)\n",
    "10. [GloVe](https://towardsdatascience.com/emnlp-what-is-glove-part-iv-e605a4c407c8)\n",
    "11. [Glove](https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "280.44px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
