{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    " A central problem in machine learning is how to make an algorithm that will perform well not just on the training data, but also on new inputs. Many strategies used in machine learning are explicitly designed to reduce the test error, possibly at the expense of increased training error. These strategies are known collectively as regularization.\n",
    "\n",
    "Therefore regularization increases training error but reduces generalization error hence more no of epochs are needed to get the desired result.Regularization helps to reduce overfitting of the model.\n",
    "\n",
    "There are many regularization techniques used some but extra term in objective function and some but extra constraint on the model.\n",
    "\n",
    " 1. L1/L2 regularizers\n",
    " 2. DropOut\n",
    " 3. Data Augmentation\n",
    " 4. Label Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1/L2 Regularizers\n",
    "L1 and L2 regularizers are some time known as weight decay.\n",
    "\n",
    "L1 Regularization works by adding an l1 norm to the cost function.\n",
    "\n",
    "$$\n",
    "L1\\ Norm : \n",
    "||X||_1 = \\sum_i |x_i|\n",
    "$$\n",
    "L2 Regularization works by adding an l2 norm to the cost function. \n",
    "\n",
    "$$\n",
    "L2\\ Norm : \n",
    "||X||_2 = \\sqrt {\\sum_i |x_i^2|}\n",
    "$$\n",
    "\n",
    "The idea behind l1 and l2 norm is smaller weight generalizes the model better so both of these norm perform some kind of weight decay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 regularization \n",
    "$$\n",
    "    C = any\\ loss\\ function  + \\frac{\\lambda}{2n}\\sum w^2\n",
    "$$\n",
    "    \n",
    "Here λ is a regularization parameter and n is the size of training data w is the weight.we are adding a sum of squares of all weights to the cost function which is scaled by λ/2n where λ > 0.\n",
    "\n",
    "The intitution behind the l2 reguarization is if cost function is increased more weights will be penalize so the nettwork prefers to learn small weights.Large weights will only be allowed if they considerably improve the first part of the cost function. Put another way, regularization can be viewed as a way of compromising between finding small weights and minimizing the original cost function. The relative importance of the two elements of the compromise depends on the value of λ: when λ is small we prefer to minimize the original cost function, but when λ is large we prefer small weights.\n",
    "\n",
    "Updating weight formulae while backprop\n",
    "$$\n",
    "w = w - \\eta \\frac{\\partial C}{\\partial w} - \\frac {\\eta \\lambda} {n} w\n",
    "$$\n",
    "\n",
    "$$\n",
    "w = \\left( 1 - \\frac{\\eta \\lambda } {n} \\right) w - \\eta \\frac{\\partial C}{\\partial w} \n",
    "$$\n",
    "\n",
    "Here \n",
    "$$\n",
    "\\left( 1 - \\frac{\\eta \\lambda } {n} \\right)\n",
    "$$\n",
    "is the rescaling factor for weights or the weight decay factor.For very small λ value it is allowing big weights and if λ value is big it is penealizing the weights.\n",
    "\n",
    "Why is this going on? Heuristically, if the cost function is unregularized, then the length of the weight vector is likely to grow, all other things being equal. Over time this can lead to the weight vector being very large indeed. This can cause the weight vector to get stuck pointing in more or less the same direction, since changes due to gradient descent only make tiny changes to the direction, when the length is long. I believe this phenomenon is making it hard for our learning algorithm to properly explore the weight space, and consequently harder to find good minima of the cost function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 regularization \n",
    "$$\n",
    "C = any\\ loss\\ function  + \\frac{\\lambda}{n}\\sum_w |w|\n",
    "$$\n",
    "\n",
    "L1 regularization is similar to l2 just the norm formulae changes from sum of squares to absolute value.\n",
    "\n",
    "Updating weight formulae while backprop\n",
    "$$\n",
    "w = w - \\eta \\frac{\\partial C}{\\partial w} - \\frac {\\eta \\lambda} {n} sign(w)\n",
    "$$\n",
    "\n",
    "sign(w) is just the sign of the weight vector +1 for positive weights and -1 for negative weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing L1 and L2 \n",
    "In both expressions the effect of regularization is to shrink the weights. This accords with our intuition that both kinds of regularization penalize large weights. But the way the weights shrink is different. In L1 regularization, the weights shrink by a constant amount toward 0. In L2 regularization, the weights shrink by an amount which is proportional to w. And so when a particular weight has a large magnitude, |w|, L1 regularization shrinks the weight much less than L2 regularization does. By contrast, when |w| is small, L1 regularization shrinks the weight much more than L2 regularization. The net result is that L1 regularization tends to concentrate the weight of the network in a relatively small number of high-importance connections, while the other weights are driven toward zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout \n",
    "Dropout is another regularization techniques which is very simple to understand.\n",
    "![](images/dropout.gif)\n",
    "So it takes a probability p and based on the value of p it randomly disables that percentage of neuron.\n",
    "\n",
    "For example if the dropout value is 0.3 on a layer. It will disable 30% neuron in the layer i.e zero the value of those neuron.\n",
    "\n",
    "While training with every batch a different set on neurons are disabled which is completely random.\n",
    "\n",
    "So why does dropout increases the robustness of the model?\n",
    "Heuristically, when we dropout different sets of neurons, it's rather like we're training different neural networks. And so the dropout procedure is like averaging the effects of a very large number of different networks. The different networks will overfit in different ways, and so, hopefully, the net effect of dropout will be to reduce overfitting.\n",
    "\n",
    "For example In cnn if the model is trained on dogs vs cats example and few particular neurons having higher weight, everytime the model witnesses the whiskers in the image it activates those neurons and we get cat. But what if those whiskers are no there then model fails significantly. so dropout forces the model to learn more attributes of the training data while training. \n",
    "\n",
    "when p = 0.5\n",
    "\n",
    "By repeating dropout over and over, our network will learn a set of weights and biases. Of course, those weights and biases will have been learnt under conditions in which half the hidden neurons were dropped out. When we actually run the full network that means that twice as many hidden neurons will be active. To compensate for that, we halve the weights outgoing from the hidden neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "The best way to make a machine learning model generalize better is to train it on more data. Of course, in practice, the amount of data we have is limited. One way to get around this problem is to create fake data and add it to the training set.\n",
    "Here Data Augmentation comes to rescue which is kind of regularization technique because now we have generated more data for the model where the image changes every time so model has to learn different attributes of the data which will lead to better generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Smoothing\n",
    "\n",
    "When we apply the cross-entropy loss to a classification task, we're expecting true labels to have 1, while the others 0. In other words, we have no doubts that the true labels are true, and the others are not. Is that always true? Maybe not. Many manual annotations are the results of multiple participants. They might have different criteria. They might make some mistakes. They are human, after all. As a result, the ground truth labels we have had perfect beliefs on are possible wrong.\n",
    "\n",
    "The impact of this on model is \n",
    "First, it may result in over-fitting:  if the  model  learns  to  assign  full  probability  to  the  ground truth label for each training example, it is not guaranteed to generalize.  Second, it encourages the differences between the largest  logit  and  all  others  to  become  large,  and  this, combined with the bounded gradient reduces the ability of the model to adapt.  Intuitively, this happens because the model becomes too confident about its predictions.\n",
    "\n",
    "One possibile solution to this is to relax our confidence on the labels. For instance, we can slighly lower the loss target values from 1 to, say, 0.9. And naturally we increase the target value of 0 for the others slightly as such. This idea is called label smoothing.\n",
    "\n",
    "[Check the result of imagenet model after applying on imagenet](https://arxiv.org/pdf/1512.00567.pdf)\n",
    "\n",
    "[Tensorflow implementation](https://github.com/Kyubyong/label_smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "\n",
    "Normalization is also a regularization technique where we equalize all the attributes of a model and bring them down to normal standard. So what actually is normal standard, let's see with a example. Suppose in the cnn model if some neuron is very active by having a high weight then all other neurons with small weight won't be able to contribute to model more so with normalization we bring down all the neurons to small scale.\n",
    "\n",
    "### Look at this image below: \n",
    "![](images/cnnfilter.jpg)\n",
    "### There is something wrong with this image. \n",
    "\n",
    "### Let's look at the greyscale version of this image to learn more. \n",
    "![](images/cnnfilter_bnw.png)\n",
    "What is wrong here is, that the amplitudes of these kernels are not \"similar\", which means even though they have learnt what do do, their contribution would not be carried forward a lot. We need to normalize them.\n",
    "\n",
    "\n",
    "### This is the effect of normalization on these kernels:\n",
    "![](images/cnnfilter_bnw_eql.png)\n",
    "\n",
    "### Normalization has now tried to equalize the kernels, i.e. pulled up the values.\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "### Let's look at these in 3d. Remember the 3D component here is just for reference, and represent the amplitudes.\n",
    "\n",
    "These are normal kernels:\n",
    "![](images/ezgif-4-024a490477.gif)\n",
    "\n",
    "### These kernels are now \"normalized\":\n",
    "![](images/ezgif-4-6ba4093f05.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistics we perform normalization by calculting z-score\n",
    "$$\n",
    "z = \\frac{x-\\mu}{\\sigma}\n",
    "$$\n",
    "which leads to the generation of bell curve \n",
    "![](https://proxy.duckduckgo.com/iu/?u=https%3A%2F%2Fwarmsouthernbreeze.files.wordpress.com%2F2012%2F06%2Fbell-curve-normal-distribution.jpg&f=1)\n",
    "\n",
    "The special property of this new normalized data is **it has zero mean and one standard deviation**\n",
    "\n",
    "There are many normalization techniques but specially in CNNs we primarly use Batch norm which is based on the above described math.\n",
    "### [Batch Normalization](https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c)\n",
    "#### why batch norm ?\n",
    "Batch normalization reduces the amount by what the hidden unit values shift around (covariance shift). To explain covariance shift, let’s have a deep network on cat detection. We train our data on only black cats’ images. So, if we now try to apply this network to data with colored cats, it is obvious; we’re not going to do well. The training set and the prediction set are both cats’ images but they differ a little bit. In other words, if an algorithm learned some X to Y mapping, and if the distribution of X changes, then we might need to retrain the learning algorithm by trying to align the distribution of X with the distribution of Y. ( Deeplearning.ai: Why Does Batch Norm Work? (C2W3L06))\n",
    "\n",
    "Also, batch normalization allows each layer of a network to learn by itself a little bit more independently of other layers.\n",
    "\n",
    "We can use higher learning rates because batch normalization makes sure that there’s no activation that’s gone really high or really low. And by that, things that previously couldn’t get to train, it will start to train.\n",
    "\n",
    "It reduces overfitting because it has a slight regularization effects. Similar to dropout, it adds some noise to each hidden layer’s activations. Therefore, if we use batch normalization, we will use less dropout, which is a good thing because we are not going to lose a lot of information. However, we should not depend only on batch normalization for regularization; we should better use it together with dropout.\n",
    "\n",
    "#### How it works?\n",
    "So we use batch norm as layer before the activation layer which normalizies the data by subtracting the batch data with batch mean and then dividing by batch standard deviation.\n",
    "\n",
    "After normalizing the layer the weights are shifted/scaled by some value for the next layer now what if the model wants to undo the normalization operation to decrease the loss, model should have some control of that, therefore we introduce two more learnable parameters gamma and beta.\n",
    "\n",
    "Where gamma can be the standard deviation and beta the mean value to perform the denormalization operation\n",
    "\n",
    "![](https://proxy.duckduckgo.com/iu/?u=https%3A%2F%2Fleonardoaraujosantos.gitbooks.io%2Fartificial-inteligence%2Fcontent%2Fimage_folder_5%2Fbatch_norm_fp.png&f=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  [Weight Norm](https://arxiv.org/pdf/1602.07868.pdf) read the paper and understand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Layer Norm]\n",
    "This is also a normalizing technique similar batch norm but it performs its operation on layers of the batch and used in RNNs.\n",
    "\n",
    "![](https://i1.wp.com/mlexplained.com/wp-content/uploads/2018/01/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88-2018-01-11-11.48.12.png)\n",
    "\n",
    "\n",
    "[For brief overview on different norms ](https://mlexplained.com/2018/01/13/weight-normalization-and-layer-normalization-explained-normalization-in-deep-learning-part-2/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
